## Transformer Machine Translation from scratch using PyTorch
##### E-mail: flavioploss@hotmail.com
##### Phone: +55 2799299-1265
##### [LinkedIn](https://www.linkedin.com/in/flavio-loss-b398a5181/)

 The Transformer architecture was presented in the 2017 paper "Attention is All You Need" and up to this day has been revolutionizing how we can leverage AI to solve real problems in an acceptable performance. One of these tasks is machine translation, which is basically translate a text from one language to another. Several algorithms and architectures were created to output translation in a human level, and a big breaktrhough on the translation task was generated by the Transformer, enabling machines to translate text in a nearly perfection basis.

 Building a Transformer from scratch requires two connected blocks: the encoder and the decoder. An encoder will deeply understand the meaning of the original text and associate weights to it, then pass its output to the decoder, which will generate text based on the inputs recieved by the two modules.

Although this is a model built from scratch, some parts are not the main focus of the article, therefore some level of help will be used by built-in modules from PyTorch. Since the main focus is what makes Transformers so good at text generation, the essencial parts that makes this possible will be built as raw as possible. 

![img-1](images/transformers.png)

#### Encoder
 
 The basic layer of a Transformer is a Self-Attention, which is composed of several computations such as matrix multiplications, that calculates the weights that the model will ajust in order to improve its translation. A self-attention step is generated by three basic variables: the queries, keysand values. Each of these values have a weight W_d associated with them, calculated using the model dimension defined by d_model. For each value, a matrix multiplication between the query and key is performed, normalizing the values and calculating the softmax of the result, and then multipliyng by the matrix v. This calculation will generate a matrix T x T, which gives the attention each token pays to the other. We also perform this calculation four times, one for each head as specified in the paper, calling this operation Multi-Head Attention.

 ![img-1](images/multi-head-attention.png)


