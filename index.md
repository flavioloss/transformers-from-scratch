## Transformer Machine Translation from scratch using PyTorch

 The Transformer architecture was presented in the 2017 paper "Attention is All You Need" and up to this day has been revolutionizing how we can leverage AI to solve real problems in an acceptable performance. One of these tasks is machine translation, which is basically translate a text from one language to another. Several algorithms and architectures were created to output translation in a human level, and a big breaktrhough on the translation task was generated by the Transformer, enabling machines to translate text in a nearly perfection basis.

 Building a Transformer from scratch requires two connected blocks: the encoder and the decoder. An encoder will deeply understand the meaning of the original text and associate weights to it, then pass its output to the decoder, which will generate text based on the inputs recieved by the two modules.

Although this is a model built from scratch, some parts are not the main focus of the article, therefore some level of help will be used by built-in modules from PyTorch. Since the main focus is what makes Transformers so good at text generation, the essencial parts that makes this possible will be built as raw as possible. 

![img-1](images/transformers.png)

#### Encoder
 
 The basic layer of a Transformer is a Self-Attention, which is composed of several computations such as matrix multiplications, that calculates the weights that the model will ajust in order to improve its translation. A self-attention step is generated by three basic variables: the queries, keysand values. Each of these values have a weight W_d associated with them, calculated using the model dimension defined by d_model. For each value, a matrix multiplication between the query and key is performed, normalizing the values and calculating the softmax of the result, and then multipliyng by the matrix v. This calculation will generate a matrix T x T, which gives the attention each token pays to the other. We also perform this calculation four times, one for each head as specified in the paper, calling this operation Multi-Head Attention.

 ![img-1](images/multi-head-attention.png)

Another import part of the self-attention is the **mask for padding**. When the text is tokenized by a pretrained **tokenizer**, a special token is used for any padding between the samples. Padding is performed because not every text input has the same length, but the model has to be provided with a matrix that has a fixed dimensionality. The padding mask will hide these tokens that are not relevant for the model, placing a zero in them, while the relevant ones are ones.

As presented in the first image, the first step in the Transformer is an Embedding layer, followed by a addition called positional encoding. This step is important because attention layers don't take into account the exact position of the word in a sentence, given that all the tokens in the self-attention layer are passed in parallel to generate the the attention weights.

For finishing up the encoder block, two normalizations layers are inserted, along with a **feed forward linear layer** and a dropout for regularization. It's important to note that the output generated by the final layer of the enocder will be used as a input on the second Multi-Head Attention layer of the decoder, more specifically, as the key and value inputs of the decoder layer.

The encoder block is repeated 6 times in the original paper, but this number is set to be a hyperparameter of the Transformer model.

#### Decoder

The following block of the Encoder uses almost the same architecture as its predecessor, with some few but important changes that are going to be addressed. 

The first one is a second mask added to the first multi-head attention layer of the decoder, which is called the **causal mask**. This mask is necessary because attention layers pass all the sentence tokens as a response in the training, and since the objective of a decoder is to predict the next token, recieving all the tokens at once would invalidate the training process. The solution comes in form of a triangular mask matrix that hides future tokens from the decoder at given timestep. Since the decoder input for training is just the translated sentence shifted right, this mask will hide tokens that the model has not seen yet, starting with a CLS token (the timestep that the mask hides the entire sentence) and forward.


##### E-mail: flavioploss@hotmail.com
##### [LinkedIn](https://www.linkedin.com/in/flavio-loss-b398a5181/)
