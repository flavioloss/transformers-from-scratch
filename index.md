### Transformer Machine Translation from scratch using PyTorch
##### E-mail: flavioploss@hotmail.com
##### Phone: +55 2799299-1265
##### [LinkedIn](https://www.linkedin.com/in/flavio-loss-b398a5181/)

 The Transformer architecture was presented in the 2017 paper "Attention is All You Need" and up to this day has been revolutionizing how we can leverage AI to solve real problems in an acceptable performance. One of these tasks is machine translation, which is basically translate a text from one language to another. Several algorithms and architectures were created to output translation in a human level, and a big breaktrhough on the translation task was generated by the Transformer, enabling machines to translate text in a nearly perfection basis.

 Building a Transformer from scratch requires two connected blocks: the encoder and the decoder. An encoder will deeply understand the meaning of the original text and associate weights to it, then pass its output to the decoder, which will generate text based on the inputs recieved by the two modules.

Although this is a model built from scratch, some parts are not the main focus of the article, therefore some level of help will be used by built-in modules from PyTorch. Since the main focus is what makes Transformers so good at text generation, the essencial parts that makes this possible will be built as raw as possible. 

![img-1](https://github.com/flavioloss/Flavio_Portfolio/blob/gh-pages/images/map_waterpump.jpeg?raw=true)
 
 
 The basic layer of a Transformer is a Self-Attention, which is composed of several computations such as matrix computations, that calculates weights that the model will learn.

![img-1](https://github.com/flavioloss/Flavio_Portfolio/blob/gh-pages/images/map_waterpump.jpeg?raw=true)
