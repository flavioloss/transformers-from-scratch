{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1LFpSTRUutM9N5HRJZZaQ6Esrf2loBvKg","authorship_tag":"ABX9TyO0u6skwcgoQCx6HBr490Ug"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"852fffed03524035bdc8bd15f3fca9aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d41c71b480da45d2a1fea111c343a0b8","IPY_MODEL_cd928b44c11f4347b501193d7e90b5f3","IPY_MODEL_297587a03bcb4681861f2dfe5bd5f5da"],"layout":"IPY_MODEL_3a200d72a58f4ad3b493da11631e62a4"}},"d41c71b480da45d2a1fea111c343a0b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f3ab8265d44acba048e66aa1042979","placeholder":"​","style":"IPY_MODEL_40b8f5a2d5524bedabe46345105eafa8","value":"100%"}},"cd928b44c11f4347b501193d7e90b5f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfc2056a78024884bad001708b9818da","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8781dcf3ab7449e08b2180e8a580a0ca","value":3}},"297587a03bcb4681861f2dfe5bd5f5da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_906854a03310467f8c5f0ae866b7bf3b","placeholder":"​","style":"IPY_MODEL_a13b43fd9ec4410fad58622c97cc7527","value":" 3/3 [00:00&lt;00:00, 97.08it/s]"}},"3a200d72a58f4ad3b493da11631e62a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4f3ab8265d44acba048e66aa1042979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40b8f5a2d5524bedabe46345105eafa8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfc2056a78024884bad001708b9818da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8781dcf3ab7449e08b2180e8a580a0ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"906854a03310467f8c5f0ae866b7bf3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a13b43fd9ec4410fad58622c97cc7527":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mvw6oGmNkJwd","executionInfo":{"status":"ok","timestamp":1689122650505,"user_tz":180,"elapsed":11241,"user":{"displayName":"flavioloss","userId":"03030109602435569562"}},"outputId":"e66a0237-d717-4db4-9654-281ed50f6474"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n","Requirement already satisfied: transformers==4.28.0 in /usr/local/lib/python3.10/dist-packages (4.28.0)\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install datasets transformers==4.28.0 torchinfo"]},{"cell_type":"code","source":["import numpy as np\n","from torch.utils.data import dataset\n","import torchinfo\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch import optim\n","import math\n","import datasets\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from datetime import datetime"],"metadata":{"id":"6yB_1C2VMxAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CausalSelfAttention(nn.Module):\n","    def __init__(self, d_k, d_model, n_heads, max_len):\n","        super().__init__()\n","\n","        self.d_k = d_k\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","\n","        # generate matrices weights\n","        self.key = nn.Linear(d_model, d_k * n_heads) # (d_model x d_k)\n","        self.query = nn.Linear(d_model, d_k * n_heads) # (d_model x d_k)\n","        self.value = nn.Linear(d_model, d_k * n_heads) # (d_model x d_k)\n","\n","        # final linear layer\n","        self.final_layer = nn.Linear(d_k * n_heads, d_model)\n","\n","        # create causal mask - weights can only access previous tokens\n","        cm = torch.tril(torch.ones(max_len, max_len))\n","        self.register_buffer(\n","            'causal_mask',\n","            cm.view(1, 1, max_len, max_len)\n","        )\n","\n","\n","    def forward(self, x, pad_mask=None):\n","        # x -> batch_size (N) x T x d_model\n","        k = self.key(x)   # N x T x h*d_k\n","        q = self.query(x) # N x T x h*d_k\n","        v = self.value(x) # N x T x h*d_v\n","\n","        N = q.shape[0] # store batch size\n","        T = q.shape[1] # store sequence dimension\n","\n","        # tranform to (N x T x h, d_k) -> (N x h x T x d_k)\n","        k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n","        q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n","        v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n","\n","        # (N x h x T x d_k) * (N x h x d_k x T) -> (N x h x T x T)\n","        attn_scores = q @ k.transpose(-1, -2) / np.sqrt(self.d_k)\n","        if pad_mask is not None:\n","            # mask is vector size 1 x T\n","            attn_scores = attn_scores.masked_fill(\n","                pad_mask[:, None, None, :] == 0, float('-inf')\n","            )\n","        attn_scores = attn_scores.masked_fill(\n","            self.causal_mask[:, :, :T, :T] == 0, float('-inf')\n","        )\n","        attn_weights = F.softmax(attn_scores, dim=-1)\n","\n","        # (N x h x T x T) * (N x h x T x d_v) -> (N x h x T x d_v)\n","        A = attn_weights @ v\n","        # reshape to (N x T x h*d_v)\n","        A = A.transpose(1, 2)\n","        A = A.contiguous().view(N, T, self.n_heads * self.d_k)\n","\n","        # (N x T x d_k * h) -> (N x T x d_model)\n","        return self.final_layer(A)"],"metadata":{"id":"66q8rKxtM03G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n","        super().__init__()\n","\n","        self.d_k = d_k\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","\n","        self.mha = CausalSelfAttention(d_k, d_model, n_heads, max_len)\n","        self.ln1 = nn.LayerNorm(d_model)\n","        # (N x T x d_model) -> (N x T)\n","        self.ann = nn.Sequential(\n","            nn.Linear(d_model, d_model * 4),\n","            nn.GELU(),\n","            nn.Linear(d_model * 4, d_model),\n","            nn.Dropout(p=dropout_prob)\n","        )\n","        self.ln2 = nn.LayerNorm(d_model)\n","        self.drop = nn.Dropout(p=dropout_prob)\n","\n","    def forward(self, x, pad_mask=None):\n","        # x = (N x T x d_model) -> (N x T x d_model)\n","        x = self.ln1(x + self.mha(x, pad_mask=pad_mask))\n","        x = self.ln2(x + self.ann(x))\n","        x = self.drop(x)\n","        return x"],"metadata":{"id":"4oYUb8h5a4Ih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_length=2048):\n","        super().__init__()\n","\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # PE(pos, 2i) = sin(pos / 10000^(2i / d_model))\n","        # PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))\n","        position = torch.arange(max_length).unsqueeze(1)\n","        exp_term = torch.arange(0, d_model, 2)\n","        pe = torch.zeros(1, max_length, d_model)\n","        div_term = torch.exp(exp_term * (-np.log(10000.0)) / d_model)\n","        pe[:, :, 0::2] = torch.sin(position * div_term)\n","        pe[:, :, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)"],"metadata":{"id":"SIYAUTPebO62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(\n","            self,\n","            d_k,\n","            max_length,\n","            vocab_size,\n","            d_model,\n","            n_heads,\n","            n_layers,\n","            dropout=0.1\n","            ):\n","        super().__init__()\n","\n","        # after tokenization -> batch_size x max_length (N x T)\n","\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        # after embedding -> batch_size x max_length x d_model (N x T x d_model)\n","\n","        self.positional_encoding = PositionalEncoding(d_model, max_length=max_length)\n","        transformer_blocks = [\n","            TransformerBlock(\n","            d_k,\n","            d_model,\n","            n_heads,\n","            max_length,\n","            dropout_prob=dropout\n","            ) for _ in range(n_layers)\n","        ]\n","        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","        self.final_layer = nn.Linear(d_model, vocab_size)\n","\n","\n","    def forward(self, x, pad_mask=None):\n","        x = self.embed(x)\n","        x = self.positional_encoding(x)\n","        for block in self.transformer_blocks:\n","            x = block(x, pad_mask=pad_mask)\n","        x = self.layer_norm(x)\n","        x = self.final_layer(x)\n","        return x"],"metadata":{"id":"ME7nStjpbT8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate decoder with dummy data\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model = Decoder(\n","    d_k=64,\n","    max_length=256,\n","    vocab_size=20000,\n","    d_model=512,\n","    n_heads=4,\n","    n_layers=2,\n",").to(device)\n","\n","\n","x_input = torch.randint(0, 20_000, (8, 256)).to(device)\n","x_pad_mask = torch.ones(8, 256).to(device)\n","x_pad_mask[:, 128:] = 0\n","\n","y = model(x_input, x_pad_mask)"],"metadata":{"id":"cp12gIADiXx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint = 'distilbert-base-cased'\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","raw_ds = datasets.load_dataset('glue', 'sst2')\n","\n","def tokenize_dataset(batch):\n","    return tokenizer(batch['sentence'], truncation=True)\n","\n","tokenized_ds = raw_ds.map(\n","    tokenize_dataset,\n","    batched=True,\n","    remove_columns=raw_ds['train'].column_names\n","    )\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","train_dataloader = DataLoader(\n","    tokenized_ds['train'],\n","    batch_size=32,\n","    shuffle=True,\n","    collate_fn=data_collator,\n",")\n","\n","model = Decoder(\n","    d_k=64,\n","    max_length=tokenizer.max_model_input_sizes[checkpoint],\n","    vocab_size=tokenizer.vocab_size,\n","    d_model=512,\n","    n_heads=4,\n","    n_layers=2,\n","    dropout=0.1,\n",").to(device)\n","\n","loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n","optimizer = optim.Adam(model.parameters())\n","\n","\n","def train_decoder(model, num_epochs, loss_fn, optimizer, train_dataloader):\n","    train_losses = []\n","    for epoch in range(1, num_epochs+1):\n","        n_train = 0\n","        train_loss = []\n","        model.train()\n","        for batch in train_dataloader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            optimizer.zero_grad()\n","            # generate shifted input tensor (N x T)\n","            # go one step left (-1) on dim T\n","            targets = batch['input_ids'].clone().detach()\n","            decoder_target = torch.roll(targets, shifts=-1, dims=1)\n","            decoder_target[:, -1] = tokenizer.pad_token_id\n","\n","            outputs = model(batch['input_ids'], batch['attention_mask'])\n","            # output: N x T\n","            loss = loss_fn(outputs.transpose(1, 2), decoder_target)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss.append(loss.item())\n","            n_train += batch['input_ids'].size(0)\n","        train_loss = np.mean(train_loss)\n","        train_losses.append(train_loss)\n","\n","        print(f'Epoch {epoch}/{num_epochs} ---> Train Loss: {train_loss:.4f}')\n","        if epoch == num_epochs:\n","            torch.save(model, '/content/drive/MyDrive/Data Science/NLP/Transformers from scratch/decoder_model.pt')\n","    return train_losses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["852fffed03524035bdc8bd15f3fca9aa","d41c71b480da45d2a1fea111c343a0b8","cd928b44c11f4347b501193d7e90b5f3","297587a03bcb4681861f2dfe5bd5f5da","3a200d72a58f4ad3b493da11631e62a4","e4f3ab8265d44acba048e66aa1042979","40b8f5a2d5524bedabe46345105eafa8","dfc2056a78024884bad001708b9818da","8781dcf3ab7449e08b2180e8a580a0ca","906854a03310467f8c5f0ae866b7bf3b","a13b43fd9ec4410fad58622c97cc7527"]},"id":"y5YZ8D9Li4Wb","executionInfo":{"status":"ok","timestamp":1689122654454,"user_tz":180,"elapsed":3333,"user":{"displayName":"flavioloss","userId":"03030109602435569562"}},"outputId":"b10368f5-545d-4a17-a5b7-dbd2f5dd1f71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"852fffed03524035bdc8bd15f3fca9aa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3cc0b0fd072f16bf.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ccf38e7f87c81821.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ae99f32f1d66a2b3.arrow\n"]}]},{"cell_type":"code","source":["train_losses = train_decoder(model, 15, loss_fn, optimizer, train_dataloader)"],"metadata":{"id":"WHovW7-bnw4d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31f455c2-b183-4c1e-f645-a5778b93e4ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15 ---> Train Loss: 4.8155\n","Epoch 2/15 ---> Train Loss: 3.5209\n","Epoch 3/15 ---> Train Loss: 2.9287\n"]}]},{"cell_type":"code","source":["def generate_text(prompt, text_len):\n","    inputs = tokenizer(prompt, return_tensors='pt')\n","    input_ids = inputs['input_ids'][:, :-1].to(device)\n","    mask = inputs['attention_mask'][:, :-1].to(device)\n","\n","    for _ in range(text_len):\n","        outputs = model(input_ids, mask)\n","        results = torch.argmax(outputs[:, -1, :], axis=-1)\n","        input_ids = torch.hstack((input_ids, results.view(1, 1)))\n","        mask = torch.ones_like(input_ids)\n","\n","        if results == tokenizer.pad_token_id:\n","            break\n","    return input_ids"],"metadata":{"id":"O07KmfgwQ6MZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = generate_text(\"it's a rather rare\", 30)\n","tokenizer.decode(input_ids[0])"],"metadata":{"id":"VRW732NhQgtW"},"execution_count":null,"outputs":[]}]}